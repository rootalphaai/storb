use std::collections::{HashMap, HashSet, VecDeque};
use std::error::Error;
use std::net::{IpAddr, Ipv4Addr};
use std::num::NonZeroUsize;
use std::path::PathBuf;
use std::sync::Arc;
use std::time::{Duration, Instant};

use dashmap::DashMap;
use libp2p::connection_limits::ConnectionLimits;
use libp2p::futures::StreamExt;
use libp2p::kad::{
    self, AddProviderOk, BootstrapOk, GetProvidersOk, GetRecordOk, PeerRecord, ProgressStep,
    PutRecordOk, QueryId, QueryResult, Quorum, Record, RecordKey,
};
use libp2p::swarm::{ConnectionId, NetworkBehaviour, Swarm, SwarmEvent};
use libp2p::{identify, identity, ping, Multiaddr, PeerId, SwarmBuilder};
use tokio::sync::{mpsc, oneshot, watch, Mutex};
use tracing::{debug, error, info, trace, warn};

use super::{db, models, store::StorbStore};
use crate::constants::{
    MAX_ESTABLISHED_INCOMING, MAX_ESTABLISHED_OUTGOING, MAX_ESTABLISHED_PER_PEER,
    MAX_PENDING_INCOMING, MAX_PENDING_OUTGOING, STORB_KAD_PROTOCOL_NAME, SWARM_RATE_LIMIT_DURATION,
    SWARM_RATE_LIMIT_REQUEST,
};
use crate::memory_db::MemoryDb;
use crate::swarm::peer_verifier;
use crate::utils::is_valid_external_addr;
use crate::AddressBook;

/// Network behaviour for Storb, combining Kademlia, Identify, and Ping.
///
/// This behaviour aggregates multiple protocols into a single behaviour that can be used
/// by a libp2p swarm.
#[derive(NetworkBehaviour)]
#[behaviour(out_event = "StorbEvent")]
pub struct StorbBehaviour {
    /// Kademlia protocol for distributed hash table operations.
    pub kademlia: kad::Behaviour<StorbStore>,

    /// Identify protocol for peer metadata exchange.
    pub identify: identify::Behaviour,

    /// Ping protocol for connectivity checks.
    pub ping: ping::Behaviour,

    pub limits: libp2p::connection_limits::Behaviour,
}

/// Enum representing events generated by the Storb network behaviour.
///
/// This enum aggregates events from individual protocols (Kademlia, Identify, Ping)
/// into a single event type.
#[derive(Debug)]
pub enum StorbEvent {
    /// Event from the Kademlia protocol.
    Kademlia(Box<kad::Event>),
    /// Event from the Identify protocol.
    Identify(Box<identify::Event>),
    /// Event from the Ping protocol.
    Ping(Box<ping::Event>),
}

impl From<kad::Event> for StorbEvent {
    fn from(event: kad::Event) -> Self {
        StorbEvent::Kademlia(Box::new(event))
    }
}

impl From<identify::Event> for StorbEvent {
    fn from(event: identify::Event) -> Self {
        StorbEvent::Identify(Box::new(event))
    }
}

impl From<ping::Event> for StorbEvent {
    fn from(event: ping::Event) -> Self {
        StorbEvent::Ping(Box::new(event))
    }
}

impl std::convert::From<std::convert::Infallible> for StorbEvent {
    fn from(_: std::convert::Infallible) -> Self {
        unreachable!()
    }
}

/// Internal enum used to associate Kademlia query IDs with their corresponding
/// response channels.
///
/// This enum is used to route responses from asynchronous DHT queries back
/// to the requesters.
enum QueryChannel {
    #[allow(dead_code)]
    Bootstrap(oneshot::Sender<Result<(), Box<dyn Error + Send + Sync>>>),
    GetRecord(
        usize,
        Vec<PeerRecord>,
        oneshot::Sender<Result<Vec<PeerRecord>, Box<dyn Error + Send + Sync>>>,
    ),
    PutRecord(oneshot::Sender<Result<(), Box<dyn Error + Send + Sync>>>),
    GetProviders(
        HashSet<PeerId>,
        oneshot::Sender<Result<HashSet<PeerId>, Box<dyn Error + Send + Sync>>>,
    ),
    StartProviding(oneshot::Sender<Result<(), Box<dyn Error + Send + Sync>>>),
}

/// DHT commands that can be issued to the StorbDHT instance.
pub enum DhtCommand {
    Put {
        key: RecordKey,
        serialized_value: Vec<u8>,
        response_tx: oneshot::Sender<Result<(), Box<dyn std::error::Error + Send + Sync>>>,
    }, // Add other commands as needed
    Get {
        key: RecordKey,
        response_tx:
            oneshot::Sender<Result<Vec<PeerRecord>, Box<dyn std::error::Error + Send + Sync>>>,
    },
    GetProviders {
        key: RecordKey,
        response_tx:
            oneshot::Sender<Result<HashSet<PeerId>, Box<dyn std::error::Error + Send + Sync>>>,
    },
    StartProviding {
        key: RecordKey,
        response_tx: oneshot::Sender<Result<(), Box<dyn std::error::Error + Send + Sync>>>,
    },
    ProcessVerificationResult {
        peer_id: PeerId,
        info: Box<identify::Info>,
        result: Result<bool, Box<dyn std::error::Error + Send + Sync>>,
    },
}

/// A Distributed Hash Table (DHT) for the Storb network.
///
/// This struct encapsulates the libp2p swarm along with mechanisms for bootstrapping,
/// query management, and record storage.
pub struct StorbDHT {
    /// The libp2p swarm managing network behaviour.
    pub swarm: Swarm<StorbBehaviour>,
    /// Watch channel sender to signal bootstrap completion.
    bootstrap_done_send: watch::Sender<bool>,
    /// Watch channel receiver to observe bootstrap completion.
    bootstrap_done_recv: watch::Receiver<bool>,
    /// Mapping of query IDs to their corresponding response channels.
    queries: Arc<Mutex<HashMap<QueryId, QueryChannel>>>,
    /// Sender for DHT commands.
    command_sender: mpsc::Sender<DhtCommand>,
    /// Channel receiver for DHT commands
    command_receiver: mpsc::Receiver<DhtCommand>,
    bootstrap_nodes: Vec<Multiaddr>,
    /// Peers that have established a connection but haven't been identified and verified yet.
    pending_verification: Arc<DashMap<PeerId, identify::Info>>,
    swarm_rate_limits: Arc<DashMap<(PeerId, IpAddr), VecDeque<Instant>>>,
    active_connections: Arc<DashMap<ConnectionId, (PeerId, IpAddr)>>,
    peer_id_to_connection_id: Arc<DashMap<PeerId, ConnectionId>>,
    shutdown_tx: watch::Sender<bool>,
}

impl StorbDHT {
    /// Creates a new StorbDHT instance.
    ///
    /// This function sets up the RocksDB store, initializes the StorbStore,
    /// configures Kademlia, Identify, and Ping protocols, and builds the libp2p swarm.
    ///
    /// # Arguments
    ///
    /// * `dht_dir` - Path to the directory for database storage.
    /// * `port` - Port number to listen on.
    /// * `keys` - Local identity keypair.
    pub fn new(
        dht_dir: PathBuf,
        mem_db: Option<Arc<MemoryDb>>,
        port: u16,
        bootstrap_peers: Option<Vec<Multiaddr>>,
        local_keypair: identity::Keypair,
        address_book: AddressBook,
    ) -> Result<(Self, mpsc::Sender<DhtCommand>), Box<dyn Error>> {
        let (command_sender, command_receiver) = mpsc::channel(32);

        assert!(port < 65535, "Invalid port number");

        // Generate a local keypair and derive our peer ID.
        let local_peer_id = libp2p::PeerId::from_public_key(&local_keypair.public());

        // Create the RocksDB store and our custom StorbStore.
        // (Propagate errors instead of panicking.)
        let db = db::RocksDBStore::new(
            db::RocksDBConfig {
                path: dht_dir,
                max_batch_delay: Duration::from_millis(10),
                max_batch_size: 100,
            },
            mem_db,
        )?;

        let store = StorbStore::new(
            std::sync::Arc::new(db),
            NonZeroUsize::new(20).ok_or("Invalid size parameter")?,
            NonZeroUsize::new(20).ok_or("Invalid size parameter")?,
        );

        // Create Kademlia with a minimal configuration.
        let kad_config = kad::Config::new(STORB_KAD_PROTOCOL_NAME);
        let kademlia = kad::Behaviour::with_config(local_peer_id, store, kad_config);

        // TODO: get rid of local peer discovery
        // Use for local peer discovery.

        // Configure Identify to gather basic peer metadata.
        let identify_config =
            identify::Config::new("0.0.1".to_string(), local_keypair.public().clone());
        let identify = identify::Behaviour::new(identify_config);

        // Use Ping to check connectivity.
        let ping = ping::Behaviour::new(ping::Config::default());
        let limits = libp2p::connection_limits::Behaviour::new(
            ConnectionLimits::default()
                .with_max_pending_incoming(Some(MAX_PENDING_INCOMING))
                .with_max_pending_outgoing(Some(MAX_PENDING_OUTGOING))
                .with_max_established_incoming(Some(MAX_ESTABLISHED_INCOMING))
                .with_max_established_outgoing(Some(MAX_ESTABLISHED_OUTGOING))
                .with_max_established_per_peer(Some(MAX_ESTABLISHED_PER_PEER)),
        );
        // Build the combined behaviour.
        let behaviour = StorbBehaviour {
            kademlia,
            identify,
            ping,
            limits,
        };

        // Build the Swarm using QUIC transport and Tokio.
        let mut swarm = SwarmBuilder::with_existing_identity(local_keypair)
            .with_tokio()
            .with_quic()
            // Using QUIC for transport.
            .with_behaviour(|_| Ok(behaviour))?
            .with_swarm_config(|cfg| cfg)
            .build();

        // Set Kademlia to server mode.
        swarm
            .behaviour_mut()
            .kademlia
            .set_mode(Some(kad::Mode::Server));

        // Listen on a QUIC multiaddress. (0.0.0.0 means all interfaces; port is ephemeral.)
        let listen_addr: Multiaddr = format!("/ip4/0.0.0.0/udp/{port}/quic-v1").parse()?;
        swarm.listen_on(listen_addr)?;

        let listeners: Vec<Multiaddr> = swarm.listeners().cloned().collect();
        info!("Listening on {:?}", listeners);

        // Create the watch channel that indicates when bootstrap is done.
        let (bootstrap_done_send, bootstrap_done_recv) = watch::channel(false);

        let mut bootstrap_nodes = Vec::new();
        if let Some(peers) = bootstrap_peers {
            for addr in &peers {
                if let Some(peer_id) = Self::extract_peer_info(addr) {
                    info!("Adding bootstrap node: {} at {}", peer_id, addr);
                    swarm
                        .behaviour_mut()
                        .kademlia
                        .add_address(&peer_id, addr.clone());
                    if let Err(err) = swarm.dial(addr.clone()) {
                        error!("Failed to dial bootstrap node {}: {:?}", addr, err);
                    }
                    bootstrap_nodes.push(addr.clone());
                } else {
                    error!("Failed to extract PeerId from bootstrap address: {}", addr);
                }
            }
        }

        if !bootstrap_nodes.is_empty() {
            match swarm.behaviour_mut().kademlia.bootstrap() {
                Ok(query_id) => {
                    debug!("Kademlia bootstrap initiated with QueryId: {:?}", query_id);
                }
                Err(e) => {
                    error!("Failed to initiate Kademlia bootstrap: {:?}", e);
                }
            }
        } else {
            bootstrap_done_send.send(true).unwrap_or_else(|e| {
                warn!("Failed to signal bootstrap completion after error: {:?}", e)
            });
        }

        let pending_verification = Arc::new(DashMap::<PeerId, identify::Info>::new());
        let swarm_rate_limits = Arc::new(DashMap::<(PeerId, IpAddr), VecDeque<Instant>>::new());
        let active_connections = Arc::new(DashMap::<ConnectionId, (PeerId, IpAddr)>::new());
        let peer_id_to_connection_id = Arc::new(DashMap::<PeerId, ConnectionId>::new());
        let (shutdown_tx, shutdown_rx) = watch::channel(false);

        let peer_verifier = peer_verifier::PeerVerifier::new(
            address_book.clone(),
            pending_verification.clone(),
            command_sender.clone(),
            shutdown_rx.clone(),
        );

        debug!("Starting peer verifier");
        peer_verifier.run();

        let dht_instance = StorbDHT {
            swarm,
            bootstrap_done_send,
            bootstrap_done_recv,
            queries: Arc::new(Mutex::new(HashMap::new())),
            command_receiver,
            command_sender,
            bootstrap_nodes,
            pending_verification,
            swarm_rate_limits,
            active_connections,
            peer_id_to_connection_id,
            shutdown_tx,
        };

        let command_sender_clone = dht_instance.command_sender.clone();
        Ok((dht_instance, command_sender_clone))
    }

    fn extract_peer_info(addr: &Multiaddr) -> Option<PeerId> {
        addr.iter().find_map(|p| match p {
            libp2p::multiaddr::Protocol::P2p(peer_id) => Some(peer_id),
            _ => None,
        })
    }

    fn extract_ip(addr: &Multiaddr) -> Option<IpAddr> {
        addr.iter().find_map(|p| match p {
            libp2p::multiaddr::Protocol::Ip4(ip) => Some(IpAddr::V4(ip)),
            libp2p::multiaddr::Protocol::Ip6(ip) => Some(IpAddr::V6(ip)),
            _ => None,
        })
    }

    fn check_rate_limit(&self, peer_id: &PeerId, ip_addr: &Ipv4Addr) -> bool {
        let now = Instant::now();
        let limit_start_time = now - SWARM_RATE_LIMIT_DURATION;
        let key = (*peer_id, IpAddr::V4(*ip_addr));
        let mut entry = self.swarm_rate_limits.entry(key).or_default();
        let timestamps = entry.value_mut();

        while let Some(ts) = timestamps.front() {
            if *ts < limit_start_time {
                timestamps.pop_front();
            } else {
                break;
            }
        }

        if timestamps.len() >= SWARM_RATE_LIMIT_REQUEST {
            warn!(%peer_id, %ip_addr, "Rate limit exceeded for peer");
            false
        } else {
            timestamps.push_back(now);
            true
        }
    }

    /// Processes a Kademlia event and routes query responses accordingly.
    ///
    /// This function inspects the provided Kademlia event, matches on the query result,
    /// and sends responses through the associated oneshot channels stored in the query map.
    async fn inject_kad_event(
        &mut self,
        event: kad::Event,
        queries: &mut HashMap<QueryId, QueryChannel>,
    ) {
        trace!("Injecting Kademlia event: {:?}", event);
        if let kad::Event::OutboundQueryProgressed {
            id,
            result,
            step: ProgressStep { last, .. },
            ..
        } = event
        {
            let query_id = id;

            match result {
                QueryResult::Bootstrap(Ok(BootstrapOk {
                    num_remaining,
                    peer,
                })) => {
                    trace!(
                        "Bootstrap succeeded with {:?} remaining and {:?}",
                        num_remaining,
                        peer
                    );

                    if !self.bootstrap_nodes.is_empty() && num_remaining == 0 {
                        // Signal that bootstrap is complete via the watch channel.
                        self.bootstrap_done_send.send(true).unwrap_or_else(|e| {
                            warn!("Failed to signal bootstrap completion: {:?}", e)
                        });
                        if let Some(QueryChannel::Bootstrap(ch)) = queries.remove(&query_id) {
                            let _ = ch.send(Ok(()));
                        }
                    }

                    trace!("No Bootstrap Nodes")
                }
                QueryResult::Bootstrap(Err(e)) => {
                    error!("Bootstrap query failed: {:?}", e); // Logged as error now
                                                               // *** Potential Fix: Signal done anyway to prevent deadlock ***
                                                               // This allows operations to proceed, though DHT might be poorly bootstrapped.
                                                               // Better long-term solutions might involve retries or checking table size.
                    warn!("Signalling bootstrap as 'done' despite error to prevent deadlock.");
                    self.bootstrap_done_send.send(true).unwrap_or_else(|e| {
                        warn!("Failed to signal bootstrap completion after error: {:?}", e)
                    });
                    // Remove query channel if you track the specific bootstrap query ID
                }
                QueryResult::GetRecord(Ok(res)) => {
                    trace!("Getting Record");
                    if let Some(QueryChannel::GetRecord(ref mut quorum, ref mut records, _)) =
                        queries.get_mut(&query_id)
                    {
                        trace!("DHT: QUERY REC WITH ID: {}", query_id);

                        if let GetRecordOk::FoundRecord(record) = res {
                            records.push(record);
                        }

                        // If this is the last response or we've met the required quorum...
                        if last || records.len() >= *quorum {
                            // Finish the query in the Kademlia behaviour.
                            if let Some(mut query) =
                                self.swarm.behaviour_mut().kademlia.query_mut(&id)
                            {
                                query.finish();
                            }
                            // Remove the query channel and send back the accumulated records.
                            if let Some(QueryChannel::GetRecord(_, records, sender)) =
                                queries.remove(&query_id)
                            {
                                let _ = sender.send(Ok(records));
                            }
                        }
                    }
                }
                QueryResult::GetRecord(Err(e)) => {
                    trace!("Query failed: {:?}", e);
                    if let Some(QueryChannel::GetRecord(_, _, ch)) = queries.remove(&query_id) {
                        let _ = ch.send(Err(e.into()));
                    }
                }
                QueryResult::PutRecord(Ok(PutRecordOk { key })) => {
                    debug!(
                        "Put record succeeded with key: {:?} and query id: {:?}",
                        key, query_id
                    );
                    if let Some(QueryChannel::PutRecord(ch)) = queries.remove(&query_id) {
                        let _ = ch.send(Ok(()));
                    }
                }
                QueryResult::PutRecord(Err(e)) => {
                    error!(
                        "ERROR: Put record failed: {:?} and query id: {:?}",
                        e, query_id
                    );
                    if let Some(QueryChannel::PutRecord(ch)) = queries.remove(&query_id) {
                        let _ = ch.send(Err(e.into()));
                    }
                }

                QueryResult::GetProviders(Ok(res)) => {
                    trace!("Getting Providers");
                    let mut finish = false;
                    if let Some(QueryChannel::GetProviders(ref mut peers, _)) =
                        queries.get_mut(&query_id)
                    {
                        match res {
                            GetProvidersOk::FoundProviders { key, providers } => {
                                trace!("Found providers for {:?}: {:?}", key, providers);
                                peers.extend(providers);
                            }
                            GetProvidersOk::FinishedWithNoAdditionalRecord { .. } => {}
                        }

                        if last {
                            finish = true;
                        }
                    }
                    if finish {
                        if let Some(QueryChannel::GetProviders(peers, ch)) =
                            queries.remove(&query_id)
                        {
                            let _ = ch.send(Ok(peers));
                        }
                    }
                }
                QueryResult::GetProviders(Err(e)) => {
                    warn!("Query failed: {:?}", e);
                    if let Some(QueryChannel::GetProviders(_, ch)) = queries.remove(&query_id) {
                        let _ = ch.send(Err(e.into()));
                    }
                }
                QueryResult::StartProviding(Ok(AddProviderOk { key })) => {
                    debug!("Start providing succeeded for {:?}", key);
                    if let Some(QueryChannel::StartProviding(ch)) = queries.remove(&query_id) {
                        let _ = ch.send(Ok(()));
                    }
                }
                QueryResult::StartProviding(Err(e)) => {
                    warn!("Start providing failed: {:?}", e);
                    if let Some(QueryChannel::StartProviding(ch)) = queries.remove(&query_id) {
                        let _ = ch.send(Err(e.into()));
                    }
                }
                _ => {}
            }
        }
    }

    fn inject_kad_incoming_query(&mut self, event: kad::Event) {
        if let kad::Event::InboundRequest {
            request: kad::InboundRequest::AddProvider { record, .. },
        } = event
        {
            trace!("AddProvider request: {:?}", record);
        }
    }

    /// Processes DHT events and commands.
    ///
    /// This asynchronous function continuously processes swarm events and DHT commands,
    /// handling peer discovery, connection management, and DHT operations.
    pub async fn process_events(&mut self) {
        let mut bootstrap_interval = tokio::time::interval(Duration::from_secs(5));
        loop {
            let pending_verification = self.pending_verification.clone();
            let active_connections = self.active_connections.clone();
            let peer_id_to_connection_id = self.peer_id_to_connection_id.clone();

            tokio::select! {
            _ = bootstrap_interval.tick() => {
                if !self.bootstrap_nodes.is_empty() {
                        let connected_count = self.bootstrap_nodes.iter()
                            .filter_map(Self::extract_peer_info)
                            .filter(|peer_id| self.swarm.connected_peers().any(|p| p == peer_id))
                            .count();

                        if connected_count < 1 {
                            // Dial any bootstrap node that's not connected.
                            for addr in self.bootstrap_nodes.clone() {
                                if let Some(peer_id) = Self::extract_peer_info(&addr) {
                                    if !self.swarm.connected_peers().any(|p| p == &peer_id) {
                                        trace!("Dialing bootstrap node: {} at {}", peer_id, addr);
                                        if let Err(err) = self.swarm.dial(addr.clone()) {
                                            error!("Failed to dial bootstrap node {}: {:?}", addr, err);
                                        }
                                        self.swarm.behaviour_mut().kademlia.add_address(&peer_id, addr);

                                    } else {
                                        trace!("Already connected to bootstrap node: {} at {}", peer_id, addr);
                                    }
                                } else {
                                    error!("Failed to extract PeerId from bootstrap address: {}", addr);
                                }
                            }
                        } else {
                            debug!(
                                "Desired number of bootstrap nodes connected ({}); stopping reconnection attempts.",
                                connected_count
                            );
                        }
                    }
                }

                Some(event) = self.swarm.next() => {
                    match event {
                        SwarmEvent::NewListenAddr { address, .. } => {
                            info!("Swarm is listening on {:?}", address);
                        }
                        SwarmEvent::ConnectionEstablished { peer_id, connection_id, endpoint, .. } => {
                            trace!(peer_id=%peer_id, connection_id=%connection_id, "Connection established with peer.");
                            let ip_addr = Self::extract_ip(endpoint.get_remote_address()).unwrap_or(IpAddr::V4(Ipv4Addr::UNSPECIFIED));
                            active_connections.insert(connection_id, (peer_id, ip_addr));
                            peer_id_to_connection_id.insert(peer_id, connection_id);
                        }
                        SwarmEvent::ConnectionClosed { peer_id, connection_id, .. } => {
                            trace!(peer_id=%peer_id, "Removing disconnected peer from Kademlia routing table.");
                            active_connections.remove(&connection_id);
                        }
                        SwarmEvent::IncomingConnection { .. } => {}
                        SwarmEvent::IncomingConnectionError { error, .. } => {
                            trace!("Incoming connection failed: {:?}", error);
                        }
                        SwarmEvent::OutgoingConnectionError { error, .. } => {
                            trace!("Outgoing connection failed: {:?}", error);
                        }
                        SwarmEvent::Dialing { .. } => {}
                        SwarmEvent::Behaviour(behaviour_event) => {
                            let mut throttled = false;

                            if let Some(conn_id) = self.get_connection_id_for_behaviour_event(&behaviour_event) {
                                if let Some(conn_info) = active_connections.get(&conn_id) {
                                    let (source_peer_id, source_ip) = *conn_info.value();

                                    let rate_limit_ok = match source_ip {
                                        IpAddr::V4(ref ip4) => self.check_rate_limit(&source_peer_id, ip4),
                                        _ => true,
                                    };
                                    if !rate_limit_ok {
                                        throttled = true;
                                        warn!(peer_id=%source_peer_id, ip=%source_ip, %conn_id, "Throttling peer");
                                        self.swarm.behaviour_mut().kademlia.remove_peer(&source_peer_id);
                                    }
                                } else {
                                    trace!(%conn_id, "Received behaviour event for untracked connection ID.");
                                }
                            } else {
                                trace!("Could not determine ConnectionId for behaviour event, skipping rate limit check.");
                            }

                            if !throttled {
                                match behaviour_event {
                                    StorbEvent::Kademlia(kad_event) => {
                                        let queries_clone = self.queries.clone();
                                        let mut queries = queries_clone.lock().await;
                                        trace!("Kademlia event received: {:?}", kad_event);
                                        self.inject_kad_event(*kad_event.clone(), &mut queries).await;
                                        self.inject_kad_incoming_query(*kad_event);
                                    }
                                    StorbEvent::Identify(identify_event) => {
                                        let event = *identify_event;
                                        trace!("Identify event: {:?}", event);
                                        if let identify::Event::Received { peer_id, info, .. } = event {
                                            pending_verification.clone().insert(peer_id, info.clone());
                                        }
                                    }
                                    StorbEvent::Ping( .. ) => {}
                                }
                            }
                        }
                        _ => {
                            trace!("Other unhandled swarm event: {:?}", event);
                        }
                    }
                }

                Some(command) = self.command_receiver.recv() => {
                    match command {
                        DhtCommand::Put {
                            key,
                            serialized_value,
                            response_tx,
                        } => {
                            debug!("Received Put command");
                            match self.handle_put(key, serialized_value, response_tx).await {
                                Ok(_) => {}
                                Err(e) => {
                                    error!("Error handling PutPiece command: {:?}", e);
                                }
                            }
                        }
                        DhtCommand::Get { key, response_tx } => {
                            debug!("Received Get command");
                            match self.handle_get(key, response_tx).await {
                                Ok(_) => {}
                                Err(e) => {
                                    error!("Error handling Get command: {:?}", e);
                                }
                            }
                        }
                        DhtCommand::StartProviding { key, response_tx } => {
                            debug!("Received StartProviding command");
                            match self.handle_start_providing(key, response_tx).await {
                                Ok(_) => {}
                                Err(e) => {
                                    error!("Error handling StartProviding command: {:?}", e);
                                }
                            }
                        }
                        DhtCommand::GetProviders { key, response_tx } => {
                            debug!("Received GetProviders command");
                            match self.handle_get_providers(key, response_tx).await {
                                Ok(_) => {}
                                Err(e) => {
                                    error!("Error handling GetProviders command: {:?}", e);
                                }
                            }
                        }
                        DhtCommand::ProcessVerificationResult {peer_id, info, result} => {
                            trace!(peer_id=%peer_id, "Processing verification result: {:?}", result);

                            match result {
                                Ok(true) => {
                                    info!(peer_id=%peer_id, "Peer verified successfully. Adding to Swarm");
                                    let mut potential_addrs = HashSet::new();
                                    let observed_addr = info.observed_addr.clone();



                                    for addr in info.listen_addrs {
                                        if is_valid_external_addr(&addr) {
                                            potential_addrs.insert(addr);
                                        }
                                    }

                                    let kademlia = &mut self.swarm.behaviour_mut().kademlia;
                                        for addr in potential_addrs {
                                            kademlia.add_address(&peer_id, addr);
                                        }
                                    kademlia.add_address(&peer_id, observed_addr);
                                    debug!(peer_id=%peer_id, "Added peer to Kademlia routing table.");
                                }
                                Ok(false) => {
                                    warn!(peer_id=%peer_id, "Peer verification failed. Disconnecting.");
                                    self.swarm.behaviour_mut().kademlia.remove_peer(&peer_id);
                                    let _ = self.swarm.disconnect_peer_id(peer_id);
                                }
                                Err(e) => {
                                    warn!(peer_id=%peer_id, "Peer verification error: {:?}", e);
                                    self.swarm.behaviour_mut().kademlia.remove_peer(&peer_id);
                                    let _ = self.swarm.disconnect_peer_id(peer_id);

                                }
                            }

                        }
                    }
                }
            }
        }
    }

    /// Attempts to extract the ConnectionId from a StorbEvent.
    /// This is needed for linking the event back to a (PeerId, IpAddr).
    fn get_connection_id_for_behaviour_event(&self, event: &StorbEvent) -> Option<ConnectionId> {
        match event {
            StorbEvent::Kademlia(kad_event) => {
                // Check specific Kademlia event types known to carry ConnectionId
                match &**kad_event {
                    kad::Event::InboundRequest { request } => {
                        // Inbound requests usually have ConnectionId
                        match request {
                            kad::InboundRequest::AddProvider { record } => {
                                if let Some(record) = record {
                                    let peer_id = record.provider;
                                    self.peer_id_to_connection_id
                                        .get(&peer_id)
                                        .map(|conn_ref| *conn_ref.value())
                                } else {
                                    None
                                }
                            }
                            kad::InboundRequest::PutRecord {
                                source: _,
                                connection,
                                record: _,
                            } => Some(*connection),
                            _ => None,
                        }
                    }
                    _ => None,
                }
            }
            StorbEvent::Identify(id_event) => match &**id_event {
                identify::Event::Received { connection_id, .. } => Some(*connection_id),
                identify::Event::Sent { connection_id, .. } => Some(*connection_id),
                identify::Event::Pushed { connection_id, .. } => Some(*connection_id),
                identify::Event::Error { connection_id, .. } => Some(*connection_id),
            },
            StorbEvent::Ping(ping_event) => {
                // Ping results usually carry ConnectionId
                let ping::Event { connection, .. } = &**ping_event;
                Some(*connection)
            }
        }
    }

    async fn handle_put(
        &mut self,
        key: RecordKey,
        value: Vec<u8>,
        response_tx: oneshot::Sender<Result<(), Box<dyn std::error::Error + Send + Sync>>>,
    ) -> Result<(), Box<dyn std::error::Error + Send + Sync>> {
        self.wait_for_bootstrap().await?;

        let record = Record {
            key: key.clone(),
            value,
            publisher: Some(*self.swarm.local_peer_id()),
            expires: None,
        };

        if !self.is_dht_healthy() {
            warn!("DHT health check failed, retrying...");
            self.ensure_dht_health().await?;
        }

        let id = self
            .swarm
            .behaviour_mut()
            .kademlia
            .put_record(record, Quorum::One) // TODO: Change to Quorum::Majority
            .map_err(|e| format!("Failed to store piece entry: {:?}", e))?;

        let mut queries = self.queries.lock().await;
        queries.insert(id, QueryChannel::PutRecord(response_tx));
        drop(queries);

        Ok(())
    }

    async fn handle_get(
        &mut self,
        key: RecordKey,
        response_tx: oneshot::Sender<
            Result<Vec<PeerRecord>, Box<dyn std::error::Error + Send + Sync>>,
        >,
    ) -> Result<(), Box<dyn std::error::Error + Send + Sync>> {
        self.wait_for_bootstrap().await?;

        let id = self.swarm.behaviour_mut().kademlia.get_record(key);

        let mut queries = self.queries.lock().await;
        queries.insert(id, QueryChannel::GetRecord(1, vec![], response_tx));
        drop(queries);

        Ok(())
    }

    async fn handle_start_providing(
        &mut self,
        key: RecordKey,
        response_tx: oneshot::Sender<Result<(), Box<dyn std::error::Error + Send + Sync>>>,
    ) -> Result<(), Box<dyn std::error::Error + Send + Sync>> {
        self.wait_for_bootstrap().await?;

        let id = self
            .swarm
            .behaviour_mut()
            .kademlia
            .start_providing(key)
            .map_err(|e| format!("Failed to register as a piece provider: {:?}", e))?;

        let mut queries = self.queries.lock().await;
        queries.insert(id, QueryChannel::StartProviding(response_tx));
        drop(queries);

        Ok(())
    }

    async fn handle_get_providers(
        &mut self,
        key: RecordKey,
        response_tx: oneshot::Sender<
            Result<HashSet<PeerId>, Box<dyn std::error::Error + Send + Sync>>,
        >,
    ) -> Result<(), Box<dyn std::error::Error + Send + Sync>> {
        self.wait_for_bootstrap().await?;

        let id = self.swarm.behaviour_mut().kademlia.get_providers(key);

        let mut queries = self.queries.lock().await;
        queries.insert(id, QueryChannel::GetProviders(HashSet::new(), response_tx));
        drop(queries);

        Ok(())
    }

    /// Inserts or updates a tracker entry in the DHT.
    ///
    /// This function serializes the provided tracker value, constructs a record,
    /// and issues a Kademlia put_record query. It waits for the query response
    /// within a specified timeout.
    pub async fn put_tracker_entry(
        command_sender: mpsc::Sender<DhtCommand>,
        infohash: RecordKey,
        value: models::TrackerDHTValue,
    ) -> Result<(), Box<dyn std::error::Error + Send + Sync>> {
        let (response_tx, response_rx) = oneshot::channel();

        let serialized_value = models::serialize_dht_value(&models::DHTValue::Tracker(value))?;

        match command_sender
            .send(DhtCommand::Put {
                key: infohash.clone(),
                serialized_value,
                response_tx,
            })
            .await
        {
            Ok(_) => {}
            Err(e) => return Err(e.into()),
        }

        match response_rx.await {
            Ok(Ok(())) => Ok(()),
            Ok(Err(e)) => Err(e),
            Err(e) => Err(Box::new(e)),
        }
    }

    /// Retrieves a tracker entry from the DHT using its infohash.
    ///
    /// This function issues a get_record query and waits for the response.
    /// If a tracker record is found and successfully deserialized, it is returned.
    pub async fn get_tracker_entry(
        command_sender: mpsc::Sender<DhtCommand>,
        infohash: RecordKey,
    ) -> Result<Option<models::TrackerDHTValue>, Box<dyn std::error::Error + Send + Sync>> {
        let (response_tx, response_rx) = oneshot::channel();
        match command_sender
            .send(DhtCommand::Get {
                key: infohash.clone(),
                response_tx,
            })
            .await
        {
            Ok(_) => {}
            Err(e) => return Err(e.into()),
        }

        match response_rx.await {
            Ok(Ok(records)) => {
                if records.is_empty() {
                    return Ok(None);
                }

                let record = match records.first() {
                    None => return Ok(None),
                    Some(r) => r,
                };

                let dht_value = models::deserialize_dht_value(&record.record.value)?;
                match dht_value {
                    models::DHTValue::Tracker(tracker) => Ok(Some(tracker)),
                    _ => Err("Record retrieved is not a tracker entry".into()),
                }
            }
            Ok(Err(e)) => Err(e),
            Err(e) => Err(Box::new(e)),
        }
    }

    /// Inserts or updates a chunk entry in the DHT.
    ///
    /// This function serializes the provided chunk value, constructs a record,
    /// and issues a Kademlia put_record query. It waits for the query response
    /// within a specified timeout.
    pub async fn put_chunk_entry(
        command_sender: mpsc::Sender<DhtCommand>,
        chunk_key: RecordKey,
        value: models::ChunkDHTValue,
    ) -> Result<(), Box<dyn std::error::Error + Send + Sync>> {
        // TODO: wait for bootstrap???
        let (response_tx, response_rx) = oneshot::channel();

        let serialized_value = models::serialize_dht_value(&models::DHTValue::Chunk(value))?;
        match command_sender
            .send(DhtCommand::Put {
                key: chunk_key,
                serialized_value,
                response_tx,
            })
            .await
        {
            Ok(_) => {}
            Err(e) => return Err(e.into()),
        }

        match response_rx.await {
            Ok(Ok(())) => Ok(()),
            Ok(Err(e)) => Err(e),
            Err(e) => Err(Box::new(e)),
        }
    }

    /// Retrieves a chunk entry from the DHT using its chunk key.
    ///
    /// This function issues a get_record query and waits for the response.
    /// If a chunk record is found and successfully deserialized, it is returned.
    pub async fn get_chunk_entry(
        command_sender: mpsc::Sender<DhtCommand>,
        chunk_key: RecordKey,
    ) -> Result<Option<models::ChunkDHTValue>, Box<dyn std::error::Error + Send + Sync>> {
        let (response_tx, response_rx) = oneshot::channel();
        match command_sender
            .send(DhtCommand::Get {
                key: chunk_key.clone(),
                response_tx,
            })
            .await
        {
            Ok(_) => {}
            Err(e) => return Err(e.into()),
        }

        match response_rx.await {
            Ok(Ok(records)) => {
                if records.is_empty() {
                    error!("No records found for chunk key: {:?}", chunk_key);
                    return Ok(None);
                }

                let record = match records.first() {
                    None => return Ok(None),
                    Some(r) => r,
                };

                let dht_value = models::deserialize_dht_value(&record.record.value)?;
                match dht_value {
                    models::DHTValue::Chunk(chunk) => Ok(Some(chunk)),
                    _ => {
                        error!("Record retrieved is not a chunk entry");
                        Err("Record retrieved is not a chunk entry".into())
                    }
                }
            }
            Ok(Err(e)) => Err(e),
            Err(e) => Err(Box::new(e)),
        }
    }

    pub async fn put_piece_entry(
        command_sender: mpsc::Sender<DhtCommand>,
        key: RecordKey,
        value: models::PieceDHTValue,
    ) -> Result<(), Box<dyn std::error::Error + Send + Sync>> {
        let (response_tx, response_rx) = oneshot::channel();

        let serialized_value = models::serialize_dht_value(&models::DHTValue::Piece(value))?;
        if command_sender.is_closed() {
            return Err("Command sender is closed".into());
        };
        match command_sender
            .send(DhtCommand::Put {
                key,
                serialized_value,
                response_tx,
            })
            .await
        {
            Ok(_) => {
                debug!("Put command sent successfully");
            }
            Err(e) => {
                error!("Failed to send Put command: {:?}", e);
                return Err(e.into());
            }
        }

        match response_rx.await {
            Ok(Ok(())) => Ok(()),
            Ok(Err(e)) => Err(e),
            Err(e) => Err(Box::new(e)),
        }
    }

    /// Retrieves a piece entry from the DHT using its piece key.
    ///
    /// This function issues a get_record query and waits for the response.
    /// If a piece record is found and successfully deserialized, it is returned.
    pub async fn get_piece_entry(
        command_sender: &mpsc::Sender<DhtCommand>,
        piece_key: RecordKey,
    ) -> Result<Option<models::PieceDHTValue>, Box<dyn std::error::Error + Send + Sync + 'static>>
    {
        let (response_tx, response_rx) = oneshot::channel();
        match command_sender
            .send(DhtCommand::Get {
                key: piece_key.clone(),
                response_tx,
            })
            .await
        {
            Ok(_) => {}
            Err(e) => return Err(e.into()),
        }

        match response_rx.await {
            Ok(Ok(records)) => {
                if records.is_empty() {
                    return Ok(None);
                }

                let record = match records.first() {
                    None => return Ok(None),
                    Some(r) => r,
                };

                let dht_value = models::deserialize_dht_value(&record.record.value)?;
                match dht_value {
                    models::DHTValue::Piece(piece) => Ok(Some(piece)),
                    _ => Err("Record retrieved is not a piece entry".into()),
                }
            }
            Ok(Err(e)) => Err(e),
            Err(e) => Err(Box::new(e)),
        }
    }

    /// Advertises that this node is providing a particular piece in the DHT.
    ///
    /// This function issues a start_providing query to announce the availability
    /// of the specified piece.
    pub async fn start_providing_piece(
        command_sender: mpsc::Sender<DhtCommand>,
        piece_key: RecordKey,
    ) -> Result<(), Box<dyn std::error::Error + Send + Sync>> {
        let (response_tx, response_rx) = oneshot::channel();
        debug!("Starting to provide piece {:?}", &piece_key);
        match command_sender
            .send(DhtCommand::StartProviding {
                key: piece_key.clone(),
                response_tx,
            })
            .await
        {
            Ok(_) => {}
            Err(e) => {
                error!("Failed to start providing piece: {}", e);
                return Err(Box::new(e));
            }
        }

        match response_rx.await {
            Ok(Ok(_)) => Ok(()),
            Ok(Err(e)) => Err(e),
            Err(e) => Err(Box::new(e)),
        }
    }

    /// Retrieves the set of peer IDs providing the specified piece.
    ///
    /// This function issues a get_providers query and waits for the response.
    pub async fn get_piece_providers(
        command_sender: &mpsc::Sender<DhtCommand>,
        piece_key: RecordKey,
    ) -> Result<HashSet<PeerId>, Box<dyn std::error::Error + Send + Sync + 'static>> {
        let (response_tx, response_rx) = oneshot::channel();
        match command_sender
            .send(DhtCommand::GetProviders {
                key: piece_key.clone(),
                response_tx,
            })
            .await
        {
            Ok(_) => {}
            Err(e) => return Err(e.into()),
        }

        match response_rx.await {
            Ok(Ok(providers)) => Ok(providers),
            Ok(Err(e)) => Err(e),
            Err(e) => Err(Box::new(e)),
        }
    }

    /// Removes a record from the DHT using its key.
    pub async fn remove_record(
        &mut self,
        key: RecordKey,
    ) -> Result<(), Box<dyn std::error::Error + Send + Sync>> {
        self.wait_for_bootstrap().await?;
        self.swarm.behaviour_mut().kademlia.remove_record(&key);
        Ok(())
    }

    /// Waits until the bootstrap process is complete.
    ///
    /// This internal function blocks until the bootstrap watch channel signals completion.
    async fn wait_for_bootstrap(&mut self) -> Result<(), Box<dyn std::error::Error + Send + Sync>> {
        debug!("Waiting for bootstrap to complete...");
        while !*self.bootstrap_done_recv.borrow() {
            trace!("Bootstrap status: waiting for change...");
            trace!(
                "Bootstrap status: {:?}",
                self.bootstrap_done_recv.changed().await?
            );
        }
        debug!("Bootstrap completed.");
        Ok(())
    }

    fn is_dht_healthy(&mut self) -> bool {
        debug!("Checking DHT health...");

        // check routing table entries
        let kbuckets = self.swarm.behaviour_mut().kademlia.kbuckets();
        // get length of the kbuckets (which is an iterator)
        let routing_table_entries = kbuckets.count();
        let connected_peers = self.swarm.connected_peers().count();
        debug!(
            "Connected peers: {}, Routing table entries: {}",
            connected_peers, routing_table_entries
        );

        connected_peers >= 1 && routing_table_entries >= 1
    }

    async fn ensure_dht_health(&mut self) -> Result<(), Box<dyn std::error::Error + Send + Sync>> {
        if !self.is_dht_healthy() {
            debug!("DHT appears unhealthy, forcing bootstrap");
            if let Err(e) = self.swarm.behaviour_mut().kademlia.bootstrap() {
                error!("Failed to bootstrap: {:?}", e);
                return Err("Failed to bootstrap DHT".into());
            }
            // Wait for bootstrap completion
            self.wait_for_bootstrap().await?;
        }
        Ok(())
    }
}

impl Drop for StorbDHT {
    /// Cleans up network connections when the StorbDHT instance is dropped.
    ///
    /// This implementation disconnects all connected peers to ensure a graceful shutdown.
    fn drop(&mut self) {
        debug!("Dropping StorbDHT connections");
        let _ = self.shutdown_tx.send(true);
        let peers: Vec<PeerId> = self.swarm.connected_peers().copied().collect();
        for (idx, conn) in peers.iter().enumerate() {
            trace!("Dropping connection {:?} to {:?}", idx, conn);
            let _ = self.swarm.disconnect_peer_id(*conn);
        }

        debug!("Dropped all connections");
    }
}
